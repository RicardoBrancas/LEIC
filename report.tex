\documentclass[a4paper,twocolumn]{article}
\usepackage[margin=2.5cm]{geometry}
\usepackage[portuguese]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{extarrows}
\usepackage{pgfplots}
\usepackage{xcolor}
\usepackage{tikz}


\usetikzlibrary{automata,positioning,calc}

\setlength{\columnsep}{0.75cm}
\pgfplotsset{compat=1.13}

\pgfplotsset{every axis/.append style={
        scale only axis,
        width=0.35\textwidth,
        height=0.21\textwidth
    },every axis legend/.append style={
        at={(0.5,-.18)},
        anchor=north
    },/tikz/every picture/.append style={
        trim axis left,
        trim axis right,
    }}

\definecolor{lightblue}{RGB}{205, 236, 247}

\title{\LARGE \textbf{Relatório do Projeto 2 de Inteligência Artificial}}
\author{Mihail Brinza \\ \scriptsize 83533 \normalsize \and Ricardo Brancas \\ \scriptsize 83557 \normalsize}

\begin{document}
    \maketitle

    \section{Métodos de Classificação}
    Para classificar as plavras escolhemos de entre outras, com recurso a \textit{cross validation}, as seguintes \textit{features}:
    \begin{enumerate}
        \item Paridade do número de caracteres;
        \item Paridade do número de caracteres ``z'';
        \item Paridade do número de vogais;
        \item Paridade do número de consoantes;
        \item Número de caracteres ``a''.
    \end{enumerate}
    Para escolher o classificador, usámos novamente \textit{cross validation} com os classificadores \textit{k-neighbors},
    com $ k = \{1,3,5,7,9\}$ e \textit{decision tree}, obtendo os resultados indicados na tabela~\ref{tab:cv1}.

    \begin{table}[ht]
        \centering
        \begin{tabular}{ l c c }
            & Conjunto 1 & Conjunto 2 \\
         \textit{1-neighbor}    & $1.0$ & $1.0$ \\
         \textit{3-neighbors}   & $1.0$ & $1.0$ \\
         \textit{5-neighbors}   & $1.0$ & $1.0$ \\
         \textit{7-neighbors}   & $1.0$ & $1.0$ \\
         \textit{9-neighbors}   & $1.0$ & $1.0$ \\
         \textit{Decision Tree} & $1.0$ & $1.0$
        \end{tabular}
        \caption{\textit{$F_1$ scores} da validação cruzada para o problema 1.}
        \label{tab:cv1}
    \end{table}

    Concluímos portanto que, para estas \textit{features}, qualquer um dos classificadores testados escolhe sempre bem dentro do
    conjunto de treino. Mais tarde verificámos que a escolha também é sempre acertada no conjunto de testes.

    Decidimos utilizar o \textit{Decision Tree Classifier} porque escolhe corretamente com maior probabilidade mesmo quando
    utilizamos \textit{features} piores.

    \section{Métodos de Regressão}
    Para escolher o método de regressão mais apropriado usamos \textit{cross validation} pontuado pelo erro quadrático
    médio, obtendo os resultados dispostos na tabela~\ref{tab:cv2}.

    \begin{table}[ht]
        \centering
        \begin{tabular}{ l c c }
            & $g_1()$ & $g_2()$ \\
         \textit{Linear Regression}                                      & $0.94$ & $816$ \\
         \textit{KR\footnotemark[1]} ($\gamma = 0.05$, $\alpha = 0.1$)   & $0.67$ & $1235$ \\
         \textit{KR\footnotemark[1]} ($\gamma = 0.05$, $\alpha = 0.01$)  & $0.34$ & \colorbox{lightblue}{$707$} \\
         \textit{KR\footnotemark[1]} ($\gamma = 0.05$, $\alpha = 0.001$) & \colorbox{lightblue}{$0.14$} & \colorbox{lightblue}{$428$} \\
         \textit{KR\footnotemark[1]} ($\gamma = 0.1$,  $\alpha = 0.1$)   & \colorbox{lightblue}{$0.23$} & $1265$ \\
         \textit{KR\footnotemark[1]} ($\gamma = 0.1$,  $\alpha = 0.01$)  & \colorbox{lightblue}{$0.10$} & $811$ \\
         \textit{KR\footnotemark[1]} ($\gamma = 0.1$,  $\alpha = 0.001$) & \colorbox{lightblue}{$0.10$} & \colorbox{lightblue}{$548$} \\
         \textit{KR\footnotemark[1]} ($\gamma = 0.2$,  $\alpha = 0.1$)   & $0.40$ & $1445$ \\
         \textit{KR\footnotemark[1]} ($\gamma = 0.2$,  $\alpha = 0.01$)  & \colorbox{lightblue}{$0.24$} & $1104$ \\
         \textit{KR\footnotemark[1]} ($\gamma = 0.2$,  $\alpha = 0.001$) & $0.66$ & \colorbox{lightblue}{$799$} \\
         \textit{KR\footnotemark[2]} (degree $= 2$)                      & $2.27$ & $3589$ \\
         \textit{KR\footnotemark[2]} (degree $= 3$)                      & $7.75$ & \colorbox{lightblue}{$0.38$} \\
         \textit{KR\footnotemark[2]} (degree $= 4$)                      & $0.93$ & \colorbox{lightblue}{$1.25$} \\
         \textit{KR\footnotemark[2]} (degree $= 5$)                      & $21.72$ & \colorbox{lightblue}{$4.39$} \\
         \textit{Decision Tree}                                          & $0.73$ & $1290$ \\
        \end{tabular}
        \caption{\textit{MSE scores} da validação cruzada para o problema 2.
        Os erros dentro do \textit{threshold} definido estão marcados a azul.}
        \label{tab:cv2}
    \end{table}
    \footnotetext[1]{\textit{Kernel Ridge} com \textit{radial basis function kernel}}
    \footnotetext[2]{\textit{Kernel Ridge} com \textit{polynomial kernel}}

    Das duas parametrizações testadas que apresentam resulados aceitáveis para ambas as funções decidimos utilizar
    a \textit{Kernel Ridge Regression} com \textit{kernels} do tipo \textit{radial basis function} e parâmetros
    \{$\gamma = 0.1$,  $\alpha = 0.001$\}. Apresentamos nas figuras~\ref{fig:reg1}~e~\ref{fig:reg2} os resultados obtidos.

    \begin{figure}[ht]
        \centering
        \begin{tikzpicture}
            \begin{axis}
                \addplot+ [only marks, mark size=0.6pt] table [x index=0, y index=1, col sep=comma] {p2_0.csv};
                \addlegendentry{Dados de teste}
                \addplot+ [mark=none,thick] table [x index=0, y index=2, col sep=comma] {p2_0.csv};
                \addlegendentry{Previsão}
                \addplot+ [only marks, mark size=3pt, mark=+] table [x index=3, y index=4, col sep=comma] {p2_0.csv};
                \addlegendentry{Dados de treino}
            \end{axis}
        \end{tikzpicture}
        \caption{Resultados obtidos para a função 1.}
        \label{fig:reg1}
    \end{figure}

    \begin{figure}[ht]
        \centering
        \begin{tikzpicture}
            \begin{axis}
                \addplot+ [only marks, mark size=0.6pt] table [x index=0, y index=1, col sep=comma] {p2_1.csv};
                \addlegendentry{Dados de teste}
                \addplot+ [mark=none,thick] table [x index=0, y index=2, col sep=comma] {p2_1.csv};
                \addlegendentry{Previsão}
                \addplot+ [only marks, mark size=3pt, mark=+] table [x index=3, y index=4, col sep=comma] {p2_1.csv};
                \addlegendentry{Dados de treino}
            \end{axis}
        \end{tikzpicture}
        \caption{Resultados obtidos para a função 2.}
        \label{fig:reg2}
    \end{figure}

    \section{Aprendizagem por Reforço}
    \subsection{Trajetórias Aprendidas}
    \subsubsection{Exemplo 1}
    $ 5_0 \xlongrightarrow{0} 6_1 \xlongrightarrow{0} 6_1 \xlongrightarrow{0} 6_1 \xlongrightarrow{0} 6_{(1)} $

    \subsubsection{Exemplo 2}
    $ 5_0 \xlongrightarrow{0} 6_1 \xlongrightarrow{0} 1_0 \xlongrightarrow{1} 0_1 \xlongrightarrow{1} 0_{(1)} $


    \subsection{Modelo do Mundo}
    \subsubsection{Exemplo 1}
    Neste primeiro exemplo o ambiente consiste numa série de quadrículas sequênciais, tal como demonstrado na
    figura~\ref{fig:amb1}, em que a ação $1$ corresponde a dar um passo para a esquerda e a ação $0$ corresponde a
    dar um passo para a direita. Tentar andar para fora dos limites não tem qualquer efeito.
    Para além disto, no estado 5 a ação 0 não é determinística; ao realizar esta ação, o agente pode ir ter ao estado
    6 ou permanecer no estado 5.

    \begin{figure}[ht]
        \centering
        \begin{tikzpicture}[shorten >=1pt,node distance=1.8cm,on grid,auto]
            \node[state,accepting] (q_0)   {0};
            \node[state] (q_1) [right=of q_0] {1};
            \node[state] (q_2) [right=of q_1]   {2};
            \node[state] (q_3) [above right=of q_2]   {3};
            \node[state] (q_4) [above left=of q_3]   {4};
            \node[state] (q_5) [left =of q_4]   {5};
            \node[state,accepting] (q_6) [left =of q_5]   {6};
            \path[->]
            (q_0) edge [bend left=20] node {0} (q_1)
                  edge [loop below] node {1} ()
            (q_1) edge [bend left=20] node {0} (q_2)
                  edge [bend left=20] node {1} (q_0)
            (q_2) edge [bend left=20] node {0} (q_3)
                  edge [bend left=20] node {1} (q_1)
            (q_3) edge [bend left=20] node {0} (q_4)
                  edge [bend left=20] node {1} (q_2)
            (q_4) edge [bend left=20] node {0} (q_5)
                  edge [bend left=20] node {1} (q_3)
            (q_5) edge [bend left=20] node {0} (q_6)
                  edge [bend left=20] node {1} (q_4)
                  edge [loop below] node {0} (q_5)
            (q_6) edge [loop below] node {0} (q_6)
                  edge [bend left=20] node {1} (q_5);
        \end{tikzpicture}
        \caption{Ambiente 1. Os nós com duplo contorno são os nós de recompensa.} \label{fig:amb1}
    \end{figure}

    Por fim determinámos que o agente é recompensado ($r = 1$) sempre que o estado
    inicial (antes da ação) é um estado limite. (i.e.~0 ou 6), não sendo recompensado
    nas outras situações ($r = 0$).

    Como tal, quando começamos no estado 5 o melhor curso de ação é andar rapidamente
    para o estado limite mais próximo, o 6, e depois mantermo-nos lá.

    \subsubsection{Exemplo 2}
    No segundo exemplo o ambiente é muito semelhante ao primeiro com exceção do estado 6. Agora quando
    tentamos andar para a direita (ação 0) nesse estado voltamos para o estado 1, tal como representado na figura~\ref{fig:amb2}.
    A função de recompensa mantem-se também inalterada, sendo 1 quando o estado inicial é o zero ou o seis, e
    0 caso contrário.

    \begin{figure}[ht]
        \centering
        \begin{tikzpicture}[shorten >=1pt,node distance=1.8cm,on grid,auto]
            \node[state,accepting] (q_0)                {0};
            \node[state]           (q_1) [right=of q_0] {1};
            \node[state]           (q_2) [right=of q_1] {2};
            \node[state]           (q_3) [right=of q_2] {3};
            \node[state]           (q_4) [above=of q_3] {4};
            \node[state]           (q_5) [left=of q_4]  {5};
            \node[state,accepting] (q_6) [left=of q_5]  {6};
            \path[->]
            (q_0) edge [bend left=20] node {0} (q_1)
                  edge [loop above] node {1} ()
            (q_1) edge [bend left=20] node {0} (q_2)
                  edge [bend left=20] node {1} (q_0)
            (q_2) edge [bend left=20] node {0} (q_3)
                  edge [bend left=20] node {1} (q_1)
            (q_3) edge [bend left=20] node {0} (q_4)
                  edge [bend left=20] node {1} (q_2)
            (q_4) edge [bend left=20] node {0} (q_5)
                  edge [bend left=20] node {1} (q_3)
            (q_5) edge [bend left=20] node {0} (q_6)
                  edge [bend left=20] node {1} (q_4)
                  edge [loop above] node {0} (q_5)
            (q_6) edge             node {0} (q_1)
                  edge [bend left=20] node {1} (q_5);
        \end{tikzpicture}
        \caption{Ambiente 2. Os nós com duplo contorno são os nós de recompensa.} \label{fig:amb2}
    \end{figure}

    Como tal [quando começamos no estado 5] já não é possível usar a estratégia anterior de ficar parado no estado 6.
    Assim, o melhor a fazer nesta situação é tentermo-nos dirigir para o estado 0, de modo a maximizar a recompensa a
    longo prazo, que é exatamente o que obtemos com o algoritmo Q-learning.

\end{document}